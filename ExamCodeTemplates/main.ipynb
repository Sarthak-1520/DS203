{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Program to do Analysis Faster Model Comparision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E3.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read data from Excel file\n",
    "file_path = 'E3-MLR3.xlsx'\n",
    "train_data = pd.read_excel(file_path, sheet_name='train')\n",
    "test_data = pd.read_excel(file_path, sheet_name='test')\n",
    "\n",
    "# Separate features and target variable in train data\n",
    "X_train = train_data.drop(columns=['y'])\n",
    "y_train = train_data['y']\n",
    "\n",
    "# Separate features and target variable in test data\n",
    "X_test = test_data.drop(columns=['y'])\n",
    "y_test = test_data['y']\n",
    "\n",
    "# Use PolynomialFeatures to create features \n",
    "deg = 6\n",
    "poly = PolynomialFeatures(degree=deg)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "####################################\n",
    "# Save the augmented data set to a file for review\n",
    "\n",
    "# Create dataframe with test data and additional features\n",
    "# Get feature names\n",
    "feature_names = poly.get_feature_names_out(X_train.columns)\n",
    "augmented_data = pd.DataFrame(X_test_poly, columns=feature_names)\n",
    "augmented_data['y'] = test_data['y']\n",
    "\n",
    "# Write dataframe to CSV\n",
    "#augmented_data.to_csv(f'aug_deg_{deg}.csv', index=False)\n",
    "####################################\n",
    "\n",
    "# Algorithms\n",
    "algorithms = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'SVM Regression': SVR(kernel='rbf'),  # Adjust kernel as needed\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'XGBoost': GradientBoostingRegressor(),\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'Neural Network': MLPRegressor(hidden_layer_sizes=[20,20], max_iter=4000),\n",
    "   # 'Elastic Net' : ElasticNet()\n",
    "}\n",
    "\n",
    "# Metric tables\n",
    "metric_table_train = pd.DataFrame()\n",
    "metric_table_test = pd.DataFrame()\n",
    "\n",
    "# Create a grid of subplots \n",
    "fig, axs = plt.subplots(len(algorithms), 4, figsize=(20, 20))\n",
    "fig_row = -1\n",
    "\n",
    "# Run the algorithms ... create metrics and plots \n",
    "for algorithm_name, algorithm in algorithms.items():\n",
    "    \n",
    "    # Train model\n",
    "    algorithm.fit(X_train_poly, y_train)\n",
    "\n",
    "    # Train predictions\n",
    "    y_train_pred = algorithm.predict(X_train_poly)\n",
    "    \n",
    "    # Test predictions\n",
    "    y_test_pred = algorithm.predict(X_test_poly)\n",
    "\n",
    "    # Train metrics\n",
    "    r2_train = algorithm.score(X_train_poly, y_train)\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    \n",
    "    # Test metrics\n",
    "    r2_test = algorithm.score(X_test_poly, y_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    # Additional metrics using statsmodels for all algorithms\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    \n",
    "    durbin_watson_stat_train = sm.stats.durbin_watson(residuals_train)\n",
    "    jb_stat_train, jb_p_value_train, _, _ = sm.stats.jarque_bera(residuals_train)\n",
    "    \n",
    "    durbin_watson_stat_test = sm.stats.durbin_watson(residuals_test)\n",
    "    jb_stat_test, jb_p_value_test, _, _ = sm.stats.jarque_bera(residuals_test)\n",
    "    \n",
    "    # Update metric tables\n",
    "    metric_table_train.at[algorithm_name, 'R-squared'] = r2_train\n",
    "    metric_table_train.at[algorithm_name, 'MSE'] = mse_train\n",
    "    metric_table_train.at[algorithm_name, 'Durbin-Watson'] = durbin_watson_stat_train\n",
    "    metric_table_train.at[algorithm_name, 'Jarque-Bera'] = jb_stat_train\n",
    "    metric_table_train.at[algorithm_name, 'JB P-value'] = jb_p_value_train\n",
    "\n",
    "    metric_table_test.at[algorithm_name, 'R-squared'] = r2_test\n",
    "    metric_table_test.at[algorithm_name, 'MSE'] = mse_test\n",
    "    metric_table_test.at[algorithm_name, 'Durbin-Watson'] = durbin_watson_stat_test\n",
    "    metric_table_test.at[algorithm_name, 'Jarque-Bera'] = jb_stat_test\n",
    "    metric_table_test.at[algorithm_name, 'JB P-value'] = jb_p_value_test\n",
    "    \n",
    "    # Create the plots\n",
    "    fig_row = fig_row+1\n",
    "    \n",
    "    axs[fig_row, 0].scatter(train_data['x1'], y_train)\n",
    "    axs[fig_row, 0].scatter(train_data['x1'], y_train_pred)\n",
    "    axs[fig_row, 0].set_title(algorithm_name + \" - Train\")\n",
    "    \n",
    "    axs[fig_row, 1].scatter(train_data['x1'], residuals_train)\n",
    "    axs[fig_row, 1].set_title(algorithm_name + \" Residuals - Train\")\n",
    "    \n",
    "    axs[fig_row, 2].scatter(test_data['x1'], y_test)\n",
    "    axs[fig_row, 2].scatter(test_data['x1'], y_test_pred)\n",
    "    axs[fig_row, 2].set_title(algorithm_name + \" - Test\")\n",
    "    \n",
    "    axs[fig_row, 3].scatter(test_data['x1'], residuals_test)\n",
    "    axs[fig_row, 3].set_title(algorithm_name + \" Residuals - Test\")\n",
    "############################\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics in tables\n",
    "print(\"Metrics - Train Data:\\n\")\n",
    "print(metric_table_train.to_string())\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "print(\"Metrics - Test Data:\\n\")\n",
    "print(metric_table_test.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Read the datasets\n",
    "datasets = ['Clusters-4-v0.csv', 'Clusters-4-v1.csv', 'Clusters-4-v2.csv']\n",
    "data = [pd.read_csv(dataset) for dataset in datasets]\n",
    "\n",
    "# Split each dataset into training and testing sets\n",
    "train_test_data = [train_test_split(dataset, test_size=0.2, random_state=42) for dataset in data]\n",
    "\n",
    "# Define the machine learning algorithms\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SVC Linear': SVC(kernel='linear', probability=True),\n",
    "    'SVC RBF': SVC(kernel='rbf', probability=True),\n",
    "    'Random Forest 1': RandomForestClassifier(min_samples_leaf=1),\n",
    "    'Random Forest 3': RandomForestClassifier(min_samples_leaf=3),\n",
    "    'Random Forest 5': RandomForestClassifier(min_samples_leaf=5),\n",
    "    'Neural Network 5': MLPClassifier(hidden_layer_sizes=(5)),\n",
    "    'Neural Network 5,5': MLPClassifier(hidden_layer_sizes=(5,5)),\n",
    "    'Neural Network 5,5,5': MLPClassifier(hidden_layer_sizes=(5,5,5)),\n",
    "    'Neural Network 10': MLPClassifier(hidden_layer_sizes=(10))\n",
    "}\n",
    "\n",
    "# Initialize the metrics dataframe\n",
    "metrics = pd.DataFrame(columns=['Algorithm', 'Dataset', 'Data', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC'])\n",
    "\n",
    "# For each algorithm\n",
    "for name, classifier in classifiers.items():\n",
    "    # For each dataset\n",
    "    for i, (X_train, X_test, y_train, y_test) in enumerate(train_test_data):\n",
    "        # Train the model\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_train_pred = classifier.predict(X_train)\n",
    "        y_test_pred = classifier.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        for data, y, y_pred in [('Train', y_train, y_train_pred), ('Test', y_test, y_test_pred)]:\n",
    "            accuracy = accuracy_score(y, y_pred)\n",
    "            precision = precision_score(y, y_pred, average=None)\n",
    "            recall = recall_score(y, y_pred, average=None)\n",
    "            f1 = f1_score(y, y_pred, average=None)\n",
    "            auc = roc_auc_score(y, y_pred)\n",
    "            \n",
    "            # Save the metrics\n",
    "            metrics = metrics.append({\n",
    "                'Algorithm': name,\n",
    "                'Dataset': datasets[i],\n",
    "                'Data': data,\n",
    "                'Accuracy': accuracy,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1-score': f1,\n",
    "                'AUC': auc\n",
    "            }, ignore_index=True)\n",
    "            \n",
    "        # Plot the classification boundaries\n",
    "        X_set, y_set = X_train, y_train\n",
    "        X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                             np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "        plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "                     alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "        plt.xlim(X1.min(), X1.max())\n",
    "        plt.ylim(X2.min(), X2.max())\n",
    "        for i, j in enumerate(np.unique(y_set)):\n",
    "            plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                        c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "        plt.title('Classifier (Training set)')\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Save the metrics into a CSV file\n",
    "metrics.to_csv('metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics generated for classification algorithms provide a way to evaluate the performance of the models. Here's a brief explanation of each metric:\n",
    "\n",
    "Accuracy: This is the ratio of the total number of correct predictions to the total number of predictions. It's a good measure when the target classes are well balanced. However, it can be misleading if the classes are imbalanced.\n",
    "\n",
    "Precision (per class and average): Precision is the ratio of true positives (correctly predicted positives) to the total predicted positives. It's a measure of a classifier's exactness. Low precision indicates a high number of false positives.\n",
    "\n",
    "Recall (per class and average): Recall (also known as sensitivity) is the ratio of true positives to the total actual positives. It's a measure of a classifier's completeness. Low recall indicates a high number of false negatives.\n",
    "\n",
    "F1-score (per class and average): The F1 score is the harmonic mean of precision and recall. It tries to find the balance between precision and recall. It's a better measure than accuracy, especially for imbalanced classes.\n",
    "\n",
    "AUC (per class and average): AUC stands for \"Area under the ROC Curve\". The ROC curve is a plot of the true positive rate (recall) against the false positive rate (1 - specificity). AUC measures the entire two-dimensional area underneath the entire ROC curve. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0 and one whose predictions are 100% correct has an AUC of 1.\n",
    "\n",
    "By comparing these metrics across different models, you can determine which model performs best for your specific task. Remember, the choice of metric depends on your business objective. For example, in a fraud detection scenario, you might want to prioritize recall (to catch as many fraud cases as possible) over precision (to avoid false alarms).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
