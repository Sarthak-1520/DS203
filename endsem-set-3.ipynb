{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc65ac-5f35-44ed-978f-fe4a106ce9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, date_format, to_date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff494460-4e42-49de-9cb0-f8271b07b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global SPARK variables\n",
    "sc = None\n",
    "ss = None\n",
    "dfr = None\n",
    "\n",
    "# Create an empty DataFrame for storing the results\n",
    "df_results = None # pd.DataFrame()\n",
    "\n",
    "# Other global variables\n",
    "mySYMBOL = None \n",
    "mySET    = None \n",
    "myKEY    = None \n",
    "mySYMBOLDataFile = None  # \"/home/hduser/spark/endsem/company_data/\"+mySYMBOL+\".csv\"\n",
    "\n",
    "def initialize_global_variables(lRoll_No, lfile, lset_no):\n",
    "    global mySYMBOL, mySET, myKEY, mySYMBOLDataFile\n",
    "    mySYMBOL = lfile.split(\".\")[0]\n",
    "    mySET = lset_no\n",
    "    myKEY = str(lRoll_No)+\"-\"+mySYMBOL+\"-\"+str(mySET)\n",
    "    mySYMBOLDataFile = \"/home/hduser/spark/endsem/company_data/\"+mySYMBOL+\".csv\"\n",
    "\n",
    "def release_global_variables():\n",
    "    global mySYMBOL, mySET, myKEY, mySYMBOLDataFile\n",
    "    del(mySYMBOL)\n",
    "    del(mySET)\n",
    "    del(myKEY)\n",
    "    del(mySYMBOLDataFile)\n",
    "    mySYMBOL = None \n",
    "    mySET    = None \n",
    "    myKEY    = None\n",
    "    mySYMBOLDataFile = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9811d9b-096d-4037-867e-13aa387d7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_variables():\n",
    "    global sc, ss, dfr\n",
    "    sc = pyspark.SparkContext(appName=\"Set-3\")\n",
    "    ss = pyspark.sql.SparkSession(sc)\n",
    "    dfr = ss.read\n",
    "#######################################################\n",
    "def release_spark_variables():\n",
    "    global ss,sc,dfr\n",
    "    ss.stop()\n",
    "    sc.stop()\n",
    "    ss = None\n",
    "    sc = None\n",
    "    dfr= None\n",
    "#######################################################\n",
    "def create_results_df():\n",
    "    global df_results\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "def add_to_results_df(row_key, col_key, avalue):\n",
    "    global df_results\n",
    "    df_results.at[row_key, col_key] = avalue\n",
    "    #print(f\"RESULT: {row_key} : {col_key} : {avalue}\")\n",
    "\n",
    "def write_results_df(ldir, lfile):\n",
    "    global df_results\n",
    "    lpath = ldir+lfile\n",
    "    print(f\"Writing results into: {lpath}\")\n",
    "    df_results.to_csv(lpath)  # write the index too\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81bcee-a31f-4b46-b19c-71399de76681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_allotted_data_into_spark():\n",
    "    global df_spark_subset_sorted, dfr\n",
    "    print(f\"Reading file into SPARK: {mySYMBOLDataFile}\")\n",
    "    df_spark_subset_sorted = dfr.csv(mySYMBOLDataFile, inferSchema=True, header=True)\n",
    "\n",
    "def release_allotted_data_spark():\n",
    "    global df_spark_subset_sorted\n",
    "    del(df_spark_subset_sorted)\n",
    "    df_spark_subset_sorted = None\n",
    "    \n",
    "def read_allotted_data_into_pd():\n",
    "    global pd_df\n",
    "    # Alternatively, create a PANDAS dataframe \n",
    "    pd_df = pd.read_csv(mySYMBOLDataFile)\n",
    "\n",
    "def release_allotted_data_pd():\n",
    "    global pd_df\n",
    "    del(pd_df)\n",
    "    pd_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a11026a-ffa1-4288-a8f8-e093cad1350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1(a) Using SPARK\n",
    "def process_1a_spark():\n",
    "    quartiles = df_spark_subset_sorted.approxQuantile(\"CLOSE\", [0.25, 0.75], 0.0001)\n",
    "    Q1, Q3 = quartiles\n",
    "    IQR = Q3 - Q1\n",
    "    add_to_results_df(myKEY, \"1a.spark\", IQR)\n",
    "    return(Q1,Q3,IQR)\n",
    "#######################################################\n",
    "# 1(a) using PANDAS\n",
    "def process_1a_pd():\n",
    "    Q1 = pd_df[\"CLOSE\"].quantile(0.25)\n",
    "    Q3 = pd_df[\"CLOSE\"].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    add_to_results_df(myKEY,\"1a.pd\",IQR)\n",
    "    return(Q1, Q3, IQR)\n",
    "#######################################################\n",
    "# 1(b) Using SPARK\n",
    "def process_1b_spark(Q1, IQR):\n",
    "    LL = Q1 - 1.5 * IQR\n",
    "    count = df_spark_subset_sorted.filter(df_spark_subset_sorted[\"CLOSE\"] < LL).count()\n",
    "    \n",
    "    add_to_results_df(myKEY,\"1b.spark\",count)\n",
    "#######################################################\n",
    "# 1(b) using Pandas\n",
    "def process_1b_pd(Q1, IQR):\n",
    "    LL = Q1 - 1.5 * IQR\n",
    "    count = pd_df[pd_df[\"CLOSE\"] < LL].shape[0]\n",
    "    \n",
    "    add_to_results_df(myKEY,\"1b.pd\",count)\n",
    "#######################################################\n",
    "# 1(c) using SPARK\n",
    "def process_1c_spark(Q3):\n",
    "    count_GT_75 = df_spark_subset_sorted.filter(df_spark_subset_sorted[\"CLOSE\"] >= Q3).count()\n",
    "    count_overall = df_spark_subset_sorted.count()\n",
    "    if(count_overall != 0):\n",
    "        pc = count_GT_75 / count_overall * 100\n",
    "    else:\n",
    "        pc = 0\n",
    "    \n",
    "    add_to_results_df(myKEY,\"1c.spark\",pc)\n",
    "#######################################################\n",
    "# 1(c) using Pandas\n",
    "def process_1c_pd(Q3):\n",
    "    count_GT_75 = pd_df[pd_df[\"CLOSE\"] >= Q3].shape[0]\n",
    "    count_overall = pd_df.shape[0]\n",
    "    if(count_overall != 0):\n",
    "        pc = count_GT_75 / count_overall * 100\n",
    "    else:\n",
    "        pc = 0    \n",
    "\n",
    "    add_to_results_df(myKEY,\"1c.pd\",pc)\n",
    "#######################################################\n",
    "# 2(a) using SPARK\n",
    "# Solution using SPARK\n",
    "def process_2a_spark():\n",
    "    list_30 = df_spark_subset_sorted.select(\"OPEN\").limit(30)\n",
    "    list_30_stdev = list_30.select(F.stddev(\"OPEN\")).collect()[0][0] \n",
    "    count = 30\n",
    "    stderr = list_30_stdev / np.sqrt(count)\n",
    "\n",
    "    add_to_results_df(myKEY,\"2a.spark\",stderr)\n",
    "    return\n",
    "#######################################################\n",
    "# 2(a) using PANDAS\n",
    "# Calculate the standard deviation of the \"HIGH\" column for the sample\n",
    "def process_2a_pd():\n",
    "    list_30_stdev = pd_df[\"OPEN\"].head(30).std()\n",
    "    count = 30\n",
    "    stderr = list_30_stdev / np.sqrt(count)\n",
    "\n",
    "    add_to_results_df(myKEY,\"2a.pd\",stderr)\n",
    "    return\n",
    "#######################################################\n",
    "from scipy.stats import norm\n",
    "\n",
    "def process_2b():\n",
    "    list_30_mean = pd_df[\"OPEN\"].head(30).mean()\n",
    "    list_30_stdev = pd_df[\"OPEN\"].head(30).std()\n",
    "    count = 30\n",
    "    \n",
    "    list_30_stderr = list_30_stdev / np.sqrt(count)\n",
    "\n",
    "    lower_limit_95 = norm.ppf(0.025, list_30_mean, list_30_stderr)\n",
    "    higher_limit_95 = norm.ppf(0.975, list_30_mean, list_30_stderr)\n",
    "    width_95 = higher_limit_95 - lower_limit_95\n",
    "\n",
    "    add_to_results_df(myKEY,\"2b\",width_95)\n",
    "    return\n",
    "#######################################################\n",
    "# 2(c)\n",
    "\n",
    "def process_2c():\n",
    "    list_30_mean = pd_df[\"OPEN\"].head(30).mean()\n",
    "    list_30_stdev = pd_df[\"OPEN\"].head(30).std()\n",
    "    count = 30\n",
    "    \n",
    "    list_30_stderr = list_30_stdev / np.sqrt(count)    \n",
    "    \n",
    "    lower_value_90_interval = norm.ppf(0.05, list_30_mean, list_30_stderr)\n",
    "    higher_value_90_interval = norm.ppf(0.95, list_30_mean, list_30_stderr)\n",
    "    width_90 = higher_value_90_interval - lower_value_90_interval\n",
    "    \n",
    "    add_to_results_df(myKEY,\"2c.1\",lower_value_90_interval)\n",
    "    add_to_results_df(myKEY,\"2c.2\",width_90)\n",
    "#######################################################\n",
    "def assign_bins(ldf, colname, boundaries): # assumption : boundaries [0, ..., ..., ..., total length]\n",
    "    # First of all, sort the dataframe in ascending values of colname\n",
    "    ldf.sort_values(by = colname, inplace=True)\n",
    "    \n",
    "    # then create a column \"BIN\" and assign the bin values sequentially \n",
    "    ldf['BIN']= \"NOVAL\"\n",
    "\n",
    "    for i in range(len(boundaries)-1):\n",
    "        bin_value = \"B\"+str(i+1)\n",
    "        ldf.iloc[boundaries[i]:boundaries[i+1], ldf.columns.get_loc('BIN')] = bin_value\n",
    "    \n",
    "    # re-sort the values back as per the original index\n",
    "    ldf.sort_index(inplace=True)\n",
    "#######################################################\n",
    "# 3(a) \n",
    "\n",
    "def process_3a():\n",
    "    count_all = len(pd_df)\n",
    "    boundaries = [0,300,600,900,count_all]\n",
    "    assign_bins(pd_df, \"HIGH\", boundaries)   \n",
    "      \n",
    "    # Print out the min and max values in each of the bins\n",
    "    for label in pd_df['BIN'].unique():\n",
    "    \n",
    "        min_val = pd_df[pd_df['BIN'] == label]['HIGH'].min()\n",
    "        max_val = pd_df[pd_df['BIN'] == label]['HIGH'].max()\n",
    "    \n",
    "        add_to_results_df(myKEY,\"3a.\"+str(label)+\".Start\", min_val)\n",
    "        add_to_results_df(myKEY,\"3a.\"+str(label)+\".End\", max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1bc9e-3f81-484b-a105-bcd111adb9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp_fn(conf_df, class_label):\n",
    "    # True Positive (TP) value is the diagonal element of the class\n",
    "    tp = conf_df.loc[class_label, class_label]\n",
    "\n",
    "    # False Negative (FN) value is the sum of the row except the diagonal element\n",
    "    fn = conf_df.loc[class_label, :].sum() - tp\n",
    "\n",
    "    return tp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be118527-95eb-439d-8f36-5bb81a759368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3(b) \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def process_3b():\n",
    "    # Create features using previous day's OPEN, HIGH, LOW, CLOSE\n",
    "    pd_df['prev_open'] = pd_df['OPEN'].shift(1)\n",
    "    pd_df['prev_high'] = pd_df['HIGH'].shift(1)\n",
    "    pd_df['prev_low'] = pd_df['LOW'].shift(1)\n",
    "    pd_df['prev_close'] = pd_df['CLOSE'].shift(1)\n",
    "    #print(pd_df.head(10))\n",
    "    \n",
    "    # Drop rows with NaN values resulting from shifting\n",
    "    pd_df_LR = pd_df.dropna()\n",
    "    \n",
    "    # Encode the categorical variable BIN\n",
    "    label_encoder = LabelEncoder()\n",
    "    pd_df_LR['BIN_encoded'] = label_encoder.fit_transform(pd_df_LR['BIN'])\n",
    "    \n",
    "    encoded_labels = label_encoder.inverse_transform([0,1,2,3])\n",
    "       \n",
    "    # Define features and target\n",
    "    features = ['prev_open', 'prev_high', 'prev_low', 'prev_close']\n",
    "    target = 'BIN_encoded'\n",
    "    \n",
    "    # Train a Logistic Regression model\n",
    "    clf = LogisticRegression(max_iter=100000)\n",
    "    clf.fit(pd_df_LR[features], pd_df_LR[target])\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(pd_df_LR[features])\n",
    "    \n",
    "    # Decode the predictions\n",
    "    y_pred_decoded = label_encoder.inverse_transform(y_pred)\n",
    "    y_true_decoded = label_encoder.inverse_transform(pd_df_LR[target])\n",
    "   \n",
    "    # Print confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_decoded, y_pred_decoded)\n",
    "    conf_df = pd.DataFrame(conf_matrix, columns=encoded_labels, index=encoded_labels)\n",
    "    #print(conf_df)\n",
    "\n",
    "    # Print classification metrics for all the BINs\n",
    "    my_classification_report = classification_report(y_true_decoded, y_pred_decoded, output_dict=True)\n",
    "    \n",
    "    tp, fn = get_tp_fn(conf_df, \"B1\")\n",
    "    add_to_results_df(myKEY,\"3b.B1.Value-1\",tp)\n",
    "    add_to_results_df(myKEY,\"3b.B1.Value-2\",fn)\n",
    "    \n",
    "    tp, fn = get_tp_fn(conf_df, \"B2\")\n",
    "    add_to_results_df(myKEY,\"3b.B2.Value-1\",tp)\n",
    "    add_to_results_df(myKEY,\"3b.B2.Value-2\",fn)\n",
    "    \n",
    "    tp, fn = get_tp_fn(conf_df, \"B3\")\n",
    "    add_to_results_df(myKEY,\"3b.B3.Value-1\",tp)\n",
    "    add_to_results_df(myKEY,\"3b.B3.Value-2\",fn)\n",
    "    \n",
    "    tp, fn = get_tp_fn(conf_df, \"B4\")\n",
    "    add_to_results_df(myKEY,\"3b.B4.Value-1\",tp)\n",
    "    add_to_results_df(myKEY,\"3b.B4.Value-2\",fn)\n",
    "    ##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a83f01-0408-416a-8d5c-f662c13bccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4(a)\n",
    "def read_all_data_into_spark():\n",
    "    \n",
    "    global df_all_spark\n",
    "\n",
    "    df_all_in_spark = dfr.csv(\"/home/hduser/spark/nsedata.csv\", inferSchema=True, header=True)\n",
    "    df_all_spark = df_all_in_spark.filter(df_all_in_spark[\"SERIES\"] == \"EQ\")\n",
    "###################\n",
    "def process_4a():\n",
    "    global df_all_spark\n",
    "\n",
    "    df_high_median = df_all_spark.groupBy(\"SYMBOL\").agg(F.median(\"HIGH\").alias(\"HIGH_MEDIAN\"))\n",
    "    \n",
    "    df_high_median_desc = df_high_median.orderBy(df_high_median[\"HIGH_MEDIAN\"].desc())\n",
    "    \n",
    "    alist = df_high_median_desc.take(2)\n",
    "    add_to_results_df(myKEY,\"4a.SYMBOL-1\",alist[0][0])\n",
    "    add_to_results_df(myKEY,\"4a.Value-1\",alist[0][1])\n",
    "    add_to_results_df(myKEY,\"4a.SYMBOL-2\",alist[1][0])\n",
    "    add_to_results_df(myKEY,\"4a.Value-2\",alist[1][1])\n",
    "    return\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f1c96-57cf-4323-b3d6-315728209362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4(b)\n",
    "def process_4b():\n",
    "    \n",
    "    global df_all_spark\n",
    "    \n",
    "    overall_median = df_all_spark.select(F.median(\"HIGH\")).collect()[0][0]\n",
    "\n",
    "    df_high_median = df_all_spark.groupBy(\"SYMBOL\").agg(F.median(\"HIGH\").alias(\"HIGH_MEDIAN\"))\n",
    "    \n",
    "    higher_than_overall_median = df_high_median.filter(df_high_median[\"HIGH_MEDIAN\"] > overall_median)\n",
    "    count = higher_than_overall_median.count()\n",
    "    \n",
    "    add_to_results_df(myKEY,\"4b\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db46504-7f25-47f0-8183-82a3312f0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4(c)\n",
    "def process_4c():\n",
    "    df_high_median = df_all_spark.groupBy(\"SYMBOL\").agg(F.median(\"HIGH\").alias(\"HIGH_MEDIAN\"))\n",
    "    \n",
    "    mySYMBOL_high_median = df_high_median.filter(df_high_median[\"SYMBOL\"] == mySYMBOL).select(\"HIGH_MEDIAN\").collect()[0][0]\n",
    "    \n",
    "    df_abs_diff = df_high_median.withColumn(\"ABS_DIFF\", F.abs(df_high_median[\"HIGH_MEDIAN\"] - mySYMBOL_high_median)).filter(df_high_median[\"SYMBOL\"] != mySYMBOL)\n",
    "    \n",
    "    closest_stock = df_abs_diff.orderBy(\"ABS_DIFF\").select(\"SYMBOL\").first()[0]\n",
    "\n",
    "    # Join the DataFrame to compare CLOSE values of closest stock to mySYMBOL\n",
    "    t1 = df_all_spark.filter(df_all_spark[\"SYMBOL\"] == mySYMBOL)\\\n",
    "                     .withColumnRenamed(\"HIGH\",\"HIGH_1\")\\\n",
    "                     .withColumnRenamed(\"SYMBOL\",\"SYMBOL_1\")\n",
    "                     \n",
    "    t2 = df_all_spark.filter(df_all_spark[\"SYMBOL\"] == closest_stock)\\\n",
    "                     .withColumnRenamed(\"HIGH\",\"HIGH_2\")\\\n",
    "                     .withColumnRenamed(\"SYMBOL\",\"SYMBOL_2\")\\\n",
    "    \n",
    "    joined_df = t1.join(t2, \"TIMESTAMP\")\n",
    "    \n",
    "    close_difference_less_than_75 = joined_df.filter(F.abs(joined_df[\"HIGH_1\"] - joined_df[\"HIGH_2\"]) < 75)\n",
    "    count_close_difference_less_than_75 = close_difference_less_than_75.count()\n",
    "    \n",
    "    add_to_results_df(myKEY,\"4c.SYMBOL\",closest_stock)\n",
    "    add_to_results_df(myKEY,\"4c.Count\",count_close_difference_less_than_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6b0cd-2908-43bd-8bdb-62cf6aacce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(lRoll_No, ldirectory, lfile):\n",
    "    # print(f\"Processing: {lfile}\")\n",
    "    initialize_global_variables(lRoll_No, lfile, \"SET3\")\n",
    "    read_all_data_into_spark()\n",
    "    read_allotted_data_into_spark()\n",
    "    read_allotted_data_into_pd()\n",
    "    ##\n",
    "    Q1_spark, Q3_spark, IQR_spark = process_1a_spark()\n",
    "    Q1_pd, Q3_pd, IQR_pd = process_1a_pd()\n",
    "    process_1b_spark(Q1_spark, IQR_spark)\n",
    "    process_1b_pd(Q1_pd, IQR_pd)\n",
    "    process_1c_spark(Q3_spark)\n",
    "    process_1c_pd(Q3_pd)\n",
    "    process_2a_spark()\n",
    "    process_2a_pd()\n",
    "    process_2b()\n",
    "    process_2c()\n",
    "    process_3a()\n",
    "    process_3b()\n",
    "    process_4a()\n",
    "    process_4b()\n",
    "    process_4c()\n",
    "    ##\n",
    "    release_allotted_data_spark()\n",
    "    release_allotted_data_pd()\n",
    "    release_global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d30914-bb08-4f42-92f2-cb8bccf33b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(arow):\n",
    "    print(f\"Processing: {arow['Roll']}, {arow['Company_Name']}, {arow['Set_No']}\")\n",
    "    directory = '/home/hduser/spark/endsem/'\n",
    "    file = arow['Company_Name'] + \".csv\"\n",
    "    process_file(arow['Roll'], directory, file)\n",
    "\n",
    "def main():\n",
    "    create_spark_variables()\n",
    "    create_results_df()\n",
    "\n",
    "    directory = '/home/hduser/spark/endsem/'\n",
    "    filepath = os.path.join(directory, 'student-and-question-data-for-endsem.xlsx')\n",
    "\n",
    "    loc_df_in = pd.read_excel(filepath)\n",
    "    df_set3 = loc_df_in[loc_df_in['Set_No']==3]\n",
    "\n",
    "    try:\n",
    "        df_set3.apply(process_row, axis=1)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "        release_allotted_data_spark()\n",
    "        release_allotted_data_pd()\n",
    "        release_global_variables()\n",
    "\n",
    "    write_results_df(directory, \"results-set3.csv\")\n",
    "    release_spark_variables()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d6579-9d32-40db-9458-d3fd45dec0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435e307-b169-4ebf-9290-55e740fb57c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25f91e-74b8-4ac7-8b69-e5b1d0d54bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed095e-df77-4c5c-aa9a-42e837b66611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530a912-6200-42be-b3b2-4f51e8475ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
